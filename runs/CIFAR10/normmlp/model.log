GeneralParams(name='runs/CIFAR10/normmlp',
              console=True,
              tensorboard=True,
              load=False,
              test=False,
              cuda=True,
              num_workers=4,
              gpu='0',
              visualize=False)
DatasetParams(dataset='CIFAR10', image_size=32)
TrainerParams(batch_size=128,
              optimizer='SGD',
              lr=0.001,
              lr_morpho=1,
              weight_decay=0,
              max_epochs=400,
              scheduler='CosineAnnealingLR',
              amp=True)
ModelParams(model='MobileNetV2-31',
            morpho_type='infinity',
            expansion=8,
            morpho_init='normal',
            relu=False,
            conv_type='norm1',
            norm_type='globalnorm')
Using cuda backend
============================================================================================================================================
Layer (type:depth-idx)                   Kernel Shape              Input Shape               Output Shape              Param #
============================================================================================================================================
MobileNetV2                              --                        [128, 3, 32, 32]          [1, 10, 128, 10]          --
├─Sequential: 1-1                        --                        [128, 3, 32, 32]          [128, 64, 8, 8]           --
│    └─InvertedResidual: 2-1             --                        [128, 3, 32, 32]          [128, 16, 32, 32]         --
│    │    └─Conv1x1: 3-1                 --                        [128, 3, 32, 32]          [128, 16, 32, 32]         48
│    │    └─BatchNorm2d: 3-2             --                        [128, 16, 32, 32]         [128, 16, 32, 32]         32
│    │    └─Identity: 3-3                --                        [128, 16, 32, 32]         [128, 16, 32, 32]         --
│    │    └─InfinityConv: 3-4            3                         [128, 16, 32, 32]         [128, 16, 32, 32]         144
│    │    └─BatchNorm2d: 3-5             --                        [128, 16, 32, 32]         [128, 16, 32, 32]         32
│    │    └─Identity: 3-6                --                        [128, 16, 32, 32]         [128, 16, 32, 32]         --
│    │    └─Conv1x1: 3-7                 --                        [128, 16, 32, 32]         [128, 16, 32, 32]         256
│    │    └─BatchNorm2d: 3-8             --                        [128, 16, 32, 32]         [128, 16, 32, 32]         32
│    └─InvertedResidual: 2-2             --                        [128, 16, 32, 32]         [128, 16, 32, 32]         --
│    │    └─Conv1x1: 3-9                 --                        [128, 16, 32, 32]         [128, 128, 32, 32]        2,048
│    │    └─BatchNorm2d: 3-10            --                        [128, 128, 32, 32]        [128, 128, 32, 32]        256
│    │    └─Identity: 3-11               --                        [128, 128, 32, 32]        [128, 128, 32, 32]        --
│    │    └─InfinityConv: 3-12           3                         [128, 128, 32, 32]        [128, 128, 32, 32]        1,152
│    │    └─BatchNorm2d: 3-13            --                        [128, 128, 32, 32]        [128, 128, 32, 32]        256
│    │    └─Identity: 3-14               --                        [128, 128, 32, 32]        [128, 128, 32, 32]        --
│    │    └─Conv1x1: 3-15                --                        [128, 128, 32, 32]        [128, 16, 32, 32]         2,048
│    │    └─BatchNorm2d: 3-16            --                        [128, 16, 32, 32]         [128, 16, 32, 32]         32
│    └─InvertedResidual: 2-3             --                        [128, 16, 32, 32]         [128, 16, 32, 32]         --
│    │    └─Conv1x1: 3-17                --                        [128, 16, 32, 32]         [128, 128, 32, 32]        2,048
│    │    └─BatchNorm2d: 3-18            --                        [128, 128, 32, 32]        [128, 128, 32, 32]        256
│    │    └─Identity: 3-19               --                        [128, 128, 32, 32]        [128, 128, 32, 32]        --
│    │    └─InfinityConv: 3-20           3                         [128, 128, 32, 32]        [128, 128, 32, 32]        1,152
│    │    └─BatchNorm2d: 3-21            --                        [128, 128, 32, 32]        [128, 128, 32, 32]        256
│    │    └─Identity: 3-22               --                        [128, 128, 32, 32]        [128, 128, 32, 32]        --
│    │    └─Conv1x1: 3-23                --                        [128, 128, 32, 32]        [128, 16, 32, 32]         2,048
│    │    └─BatchNorm2d: 3-24            --                        [128, 16, 32, 32]         [128, 16, 32, 32]         32
│    └─InvertedResidual: 2-4             --                        [128, 16, 32, 32]         [128, 16, 32, 32]         --
│    │    └─Conv1x1: 3-25                --                        [128, 16, 32, 32]         [128, 128, 32, 32]        2,048
│    │    └─BatchNorm2d: 3-26            --                        [128, 128, 32, 32]        [128, 128, 32, 32]        256
│    │    └─Identity: 3-27               --                        [128, 128, 32, 32]        [128, 128, 32, 32]        --
│    │    └─InfinityConv: 3-28           3                         [128, 128, 32, 32]        [128, 128, 32, 32]        1,152
│    │    └─BatchNorm2d: 3-29            --                        [128, 128, 32, 32]        [128, 128, 32, 32]        256
│    │    └─Identity: 3-30               --                        [128, 128, 32, 32]        [128, 128, 32, 32]        --
│    │    └─Conv1x1: 3-31                --                        [128, 128, 32, 32]        [128, 16, 32, 32]         2,048
│    │    └─BatchNorm2d: 3-32            --                        [128, 16, 32, 32]         [128, 16, 32, 32]         32
│    └─InvertedResidual: 2-5             --                        [128, 16, 32, 32]         [128, 32, 16, 16]         --
│    │    └─Conv1x1: 3-33                --                        [128, 16, 32, 32]         [128, 128, 32, 32]        2,048
│    │    └─BatchNorm2d: 3-34            --                        [128, 128, 32, 32]        [128, 128, 32, 32]        256
│    │    └─Identity: 3-35               --                        [128, 128, 32, 32]        [128, 128, 32, 32]        --
│    │    └─InfinityConv: 3-36           3                         [128, 128, 32, 32]        [128, 128, 16, 16]        1,152
│    │    └─BatchNorm2d: 3-37            --                        [128, 128, 16, 16]        [128, 128, 16, 16]        256
│    │    └─Identity: 3-38               --                        [128, 128, 16, 16]        [128, 128, 16, 16]        --
│    │    └─Conv1x1: 3-39                --                        [128, 128, 16, 16]        [128, 32, 16, 16]         4,096
│    │    └─BatchNorm2d: 3-40            --                        [128, 32, 16, 16]         [128, 32, 16, 16]         64
│    └─InvertedResidual: 2-6             --                        [128, 32, 16, 16]         [128, 32, 16, 16]         --
│    │    └─Conv1x1: 3-41                --                        [128, 32, 16, 16]         [128, 256, 16, 16]        8,192
│    │    └─BatchNorm2d: 3-42            --                        [128, 256, 16, 16]        [128, 256, 16, 16]        512
│    │    └─Identity: 3-43               --                        [128, 256, 16, 16]        [128, 256, 16, 16]        --
│    │    └─InfinityConv: 3-44           3                         [128, 256, 16, 16]        [128, 256, 16, 16]        2,304
│    │    └─BatchNorm2d: 3-45            --                        [128, 256, 16, 16]        [128, 256, 16, 16]        512
│    │    └─Identity: 3-46               --                        [128, 256, 16, 16]        [128, 256, 16, 16]        --
│    │    └─Conv1x1: 3-47                --                        [128, 256, 16, 16]        [128, 32, 16, 16]         8,192
│    │    └─BatchNorm2d: 3-48            --                        [128, 32, 16, 16]         [128, 32, 16, 16]         64
│    └─InvertedResidual: 2-7             --                        [128, 32, 16, 16]         [128, 32, 16, 16]         --
│    │    └─Conv1x1: 3-49                --                        [128, 32, 16, 16]         [128, 256, 16, 16]        8,192
│    │    └─BatchNorm2d: 3-50            --                        [128, 256, 16, 16]        [128, 256, 16, 16]        512
│    │    └─Identity: 3-51               --                        [128, 256, 16, 16]        [128, 256, 16, 16]        --
│    │    └─InfinityConv: 3-52           3                         [128, 256, 16, 16]        [128, 256, 16, 16]        2,304
│    │    └─BatchNorm2d: 3-53            --                        [128, 256, 16, 16]        [128, 256, 16, 16]        512
│    │    └─Identity: 3-54               --                        [128, 256, 16, 16]        [128, 256, 16, 16]        --
│    │    └─Conv1x1: 3-55                --                        [128, 256, 16, 16]        [128, 32, 16, 16]         8,192
│    │    └─BatchNorm2d: 3-56            --                        [128, 32, 16, 16]         [128, 32, 16, 16]         64
│    └─InvertedResidual: 2-8             --                        [128, 32, 16, 16]         [128, 64, 8, 8]           --
│    │    └─Conv1x1: 3-57                --                        [128, 32, 16, 16]         [128, 256, 16, 16]        8,192
│    │    └─BatchNorm2d: 3-58            --                        [128, 256, 16, 16]        [128, 256, 16, 16]        512
│    │    └─Identity: 3-59               --                        [128, 256, 16, 16]        [128, 256, 16, 16]        --
│    │    └─InfinityConv: 3-60           3                         [128, 256, 16, 16]        [128, 256, 8, 8]          2,304
│    │    └─BatchNorm2d: 3-61            --                        [128, 256, 8, 8]          [128, 256, 8, 8]          512
│    │    └─Identity: 3-62               --                        [128, 256, 8, 8]          [128, 256, 8, 8]          --
│    │    └─Conv1x1: 3-63                --                        [128, 256, 8, 8]          [128, 64, 8, 8]           16,384
│    │    └─BatchNorm2d: 3-64            --                        [128, 64, 8, 8]           [128, 64, 8, 8]           128
│    └─InvertedResidual: 2-9             --                        [128, 64, 8, 8]           [128, 64, 8, 8]           --
│    │    └─Conv1x1: 3-65                --                        [128, 64, 8, 8]           [128, 512, 8, 8]          32,768
│    │    └─BatchNorm2d: 3-66            --                        [128, 512, 8, 8]          [128, 512, 8, 8]          1,024
│    │    └─Identity: 3-67               --                        [128, 512, 8, 8]          [128, 512, 8, 8]          --
│    │    └─InfinityConv: 3-68           3                         [128, 512, 8, 8]          [128, 512, 8, 8]          4,608
│    │    └─BatchNorm2d: 3-69            --                        [128, 512, 8, 8]          [128, 512, 8, 8]          1,024
│    │    └─Identity: 3-70               --                        [128, 512, 8, 8]          [128, 512, 8, 8]          --
│    │    └─Conv1x1: 3-71                --                        [128, 512, 8, 8]          [128, 64, 8, 8]           32,768
│    │    └─BatchNorm2d: 3-72            --                        [128, 64, 8, 8]           [128, 64, 8, 8]           128
│    └─InvertedResidual: 2-10            --                        [128, 64, 8, 8]           [128, 64, 8, 8]           --
│    │    └─Conv1x1: 3-73                --                        [128, 64, 8, 8]           [128, 512, 8, 8]          32,768
│    │    └─BatchNorm2d: 3-74            --                        [128, 512, 8, 8]          [128, 512, 8, 8]          1,024
│    │    └─Identity: 3-75               --                        [128, 512, 8, 8]          [128, 512, 8, 8]          --
│    │    └─InfinityConv: 3-76           3                         [128, 512, 8, 8]          [128, 512, 8, 8]          4,608
│    │    └─BatchNorm2d: 3-77            --                        [128, 512, 8, 8]          [128, 512, 8, 8]          1,024
│    │    └─Identity: 3-78               --                        [128, 512, 8, 8]          [128, 512, 8, 8]          --
│    │    └─Conv1x1: 3-79                --                        [128, 512, 8, 8]          [128, 64, 8, 8]           32,768
│    │    └─BatchNorm2d: 3-80            --                        [128, 64, 8, 8]           [128, 64, 8, 8]           128
├─Sequential: 1-2                        --                        [128, 64]                 [1, 10, 128, 10]          --
│    └─Dropout: 2-11                     --                        [128, 64]                 [128, 64]                 --
│    └─NormLinear: 2-12                  --                        [128, 64]                 [1, 10, 128, 10]          650
============================================================================================================================================
Total params: 238,666
Trainable params: 238,666
Non-trainable params: 0
Total mult-adds (G): 4.60
============================================================================================================================================
Input size (MB): 1.57
Forward/backward pass size (MB): 3196.16
Params size (MB): 0.95
Estimated Total Size (MB): 3198.69
============================================================================================================================================GeneralParams(name='runs/CIFAR10/normmlp',
              console=True,
              tensorboard=True,
              load=False,
              test=False,
              cuda=True,
              num_workers=4,
              gpu='0',
              visualize=False)
DatasetParams(dataset='CIFAR10', image_size=32)
TrainerParams(batch_size=128,
              optimizer='SGD',
              lr=0.001,
              lr_morpho=1,
              weight_decay=0,
              max_epochs=400,
              scheduler='CosineAnnealingLR',
              amp=True)
ModelParams(model='MobileNetV2-31',
            morpho_type='infinity',
            expansion=8,
            morpho_init='normal',
            relu=False,
            conv_type='norm1',
            norm_type='globalnorm')
Using cuda backend
============================================================================================================================================
Layer (type:depth-idx)                   Kernel Shape              Input Shape               Output Shape              Param #
============================================================================================================================================
MobileNetV2                              --                        [128, 3, 32, 32]          [128, 10]                 --
├─Sequential: 1-1                        --                        [128, 3, 32, 32]          [128, 64, 8, 8]           --
│    └─InvertedResidual: 2-1             --                        [128, 3, 32, 32]          [128, 16, 32, 32]         --
│    │    └─Conv1x1: 3-1                 --                        [128, 3, 32, 32]          [128, 16, 32, 32]         48
│    │    └─BatchNorm2d: 3-2             --                        [128, 16, 32, 32]         [128, 16, 32, 32]         32
│    │    └─Identity: 3-3                --                        [128, 16, 32, 32]         [128, 16, 32, 32]         --
│    │    └─InfinityConv: 3-4            3                         [128, 16, 32, 32]         [128, 16, 32, 32]         144
│    │    └─BatchNorm2d: 3-5             --                        [128, 16, 32, 32]         [128, 16, 32, 32]         32
│    │    └─Identity: 3-6                --                        [128, 16, 32, 32]         [128, 16, 32, 32]         --
│    │    └─Conv1x1: 3-7                 --                        [128, 16, 32, 32]         [128, 16, 32, 32]         256
│    │    └─BatchNorm2d: 3-8             --                        [128, 16, 32, 32]         [128, 16, 32, 32]         32
│    └─InvertedResidual: 2-2             --                        [128, 16, 32, 32]         [128, 16, 32, 32]         --
│    │    └─Conv1x1: 3-9                 --                        [128, 16, 32, 32]         [128, 128, 32, 32]        2,048
│    │    └─BatchNorm2d: 3-10            --                        [128, 128, 32, 32]        [128, 128, 32, 32]        256
│    │    └─Identity: 3-11               --                        [128, 128, 32, 32]        [128, 128, 32, 32]        --
│    │    └─InfinityConv: 3-12           3                         [128, 128, 32, 32]        [128, 128, 32, 32]        1,152
│    │    └─BatchNorm2d: 3-13            --                        [128, 128, 32, 32]        [128, 128, 32, 32]        256
│    │    └─Identity: 3-14               --                        [128, 128, 32, 32]        [128, 128, 32, 32]        --
│    │    └─Conv1x1: 3-15                --                        [128, 128, 32, 32]        [128, 16, 32, 32]         2,048
│    │    └─BatchNorm2d: 3-16            --                        [128, 16, 32, 32]         [128, 16, 32, 32]         32
│    └─InvertedResidual: 2-3             --                        [128, 16, 32, 32]         [128, 16, 32, 32]         --
│    │    └─Conv1x1: 3-17                --                        [128, 16, 32, 32]         [128, 128, 32, 32]        2,048
│    │    └─BatchNorm2d: 3-18            --                        [128, 128, 32, 32]        [128, 128, 32, 32]        256
│    │    └─Identity: 3-19               --                        [128, 128, 32, 32]        [128, 128, 32, 32]        --
│    │    └─InfinityConv: 3-20           3                         [128, 128, 32, 32]        [128, 128, 32, 32]        1,152
│    │    └─BatchNorm2d: 3-21            --                        [128, 128, 32, 32]        [128, 128, 32, 32]        256
│    │    └─Identity: 3-22               --                        [128, 128, 32, 32]        [128, 128, 32, 32]        --
│    │    └─Conv1x1: 3-23                --                        [128, 128, 32, 32]        [128, 16, 32, 32]         2,048
│    │    └─BatchNorm2d: 3-24            --                        [128, 16, 32, 32]         [128, 16, 32, 32]         32
│    └─InvertedResidual: 2-4             --                        [128, 16, 32, 32]         [128, 16, 32, 32]         --
│    │    └─Conv1x1: 3-25                --                        [128, 16, 32, 32]         [128, 128, 32, 32]        2,048
│    │    └─BatchNorm2d: 3-26            --                        [128, 128, 32, 32]        [128, 128, 32, 32]        256
│    │    └─Identity: 3-27               --                        [128, 128, 32, 32]        [128, 128, 32, 32]        --
│    │    └─InfinityConv: 3-28           3                         [128, 128, 32, 32]        [128, 128, 32, 32]        1,152
│    │    └─BatchNorm2d: 3-29            --                        [128, 128, 32, 32]        [128, 128, 32, 32]        256
│    │    └─Identity: 3-30               --                        [128, 128, 32, 32]        [128, 128, 32, 32]        --
│    │    └─Conv1x1: 3-31                --                        [128, 128, 32, 32]        [128, 16, 32, 32]         2,048
│    │    └─BatchNorm2d: 3-32            --                        [128, 16, 32, 32]         [128, 16, 32, 32]         32
│    └─InvertedResidual: 2-5             --                        [128, 16, 32, 32]         [128, 32, 16, 16]         --
│    │    └─Conv1x1: 3-33                --                        [128, 16, 32, 32]         [128, 128, 32, 32]        2,048
│    │    └─BatchNorm2d: 3-34            --                        [128, 128, 32, 32]        [128, 128, 32, 32]        256
│    │    └─Identity: 3-35               --                        [128, 128, 32, 32]        [128, 128, 32, 32]        --
│    │    └─InfinityConv: 3-36           3                         [128, 128, 32, 32]        [128, 128, 16, 16]        1,152
│    │    └─BatchNorm2d: 3-37            --                        [128, 128, 16, 16]        [128, 128, 16, 16]        256
│    │    └─Identity: 3-38               --                        [128, 128, 16, 16]        [128, 128, 16, 16]        --
│    │    └─Conv1x1: 3-39                --                        [128, 128, 16, 16]        [128, 32, 16, 16]         4,096
│    │    └─BatchNorm2d: 3-40            --                        [128, 32, 16, 16]         [128, 32, 16, 16]         64
│    └─InvertedResidual: 2-6             --                        [128, 32, 16, 16]         [128, 32, 16, 16]         --
│    │    └─Conv1x1: 3-41                --                        [128, 32, 16, 16]         [128, 256, 16, 16]        8,192
│    │    └─BatchNorm2d: 3-42            --                        [128, 256, 16, 16]        [128, 256, 16, 16]        512
│    │    └─Identity: 3-43               --                        [128, 256, 16, 16]        [128, 256, 16, 16]        --
│    │    └─InfinityConv: 3-44           3                         [128, 256, 16, 16]        [128, 256, 16, 16]        2,304
│    │    └─BatchNorm2d: 3-45            --                        [128, 256, 16, 16]        [128, 256, 16, 16]        512
│    │    └─Identity: 3-46               --                        [128, 256, 16, 16]        [128, 256, 16, 16]        --
│    │    └─Conv1x1: 3-47                --                        [128, 256, 16, 16]        [128, 32, 16, 16]         8,192
│    │    └─BatchNorm2d: 3-48            --                        [128, 32, 16, 16]         [128, 32, 16, 16]         64
│    └─InvertedResidual: 2-7             --                        [128, 32, 16, 16]         [128, 32, 16, 16]         --
│    │    └─Conv1x1: 3-49                --                        [128, 32, 16, 16]         [128, 256, 16, 16]        8,192
│    │    └─BatchNorm2d: 3-50            --                        [128, 256, 16, 16]        [128, 256, 16, 16]        512
│    │    └─Identity: 3-51               --                        [128, 256, 16, 16]        [128, 256, 16, 16]        --
│    │    └─InfinityConv: 3-52           3                         [128, 256, 16, 16]        [128, 256, 16, 16]        2,304
│    │    └─BatchNorm2d: 3-53            --                        [128, 256, 16, 16]        [128, 256, 16, 16]        512
│    │    └─Identity: 3-54               --                        [128, 256, 16, 16]        [128, 256, 16, 16]        --
│    │    └─Conv1x1: 3-55                --                        [128, 256, 16, 16]        [128, 32, 16, 16]         8,192
│    │    └─BatchNorm2d: 3-56            --                        [128, 32, 16, 16]         [128, 32, 16, 16]         64
│    └─InvertedResidual: 2-8             --                        [128, 32, 16, 16]         [128, 64, 8, 8]           --
│    │    └─Conv1x1: 3-57                --                        [128, 32, 16, 16]         [128, 256, 16, 16]        8,192
│    │    └─BatchNorm2d: 3-58            --                        [128, 256, 16, 16]        [128, 256, 16, 16]        512
│    │    └─Identity: 3-59               --                        [128, 256, 16, 16]        [128, 256, 16, 16]        --
│    │    └─InfinityConv: 3-60           3                         [128, 256, 16, 16]        [128, 256, 8, 8]          2,304
│    │    └─BatchNorm2d: 3-61            --                        [128, 256, 8, 8]          [128, 256, 8, 8]          512
│    │    └─Identity: 3-62               --                        [128, 256, 8, 8]          [128, 256, 8, 8]          --
│    │    └─Conv1x1: 3-63                --                        [128, 256, 8, 8]          [128, 64, 8, 8]           16,384
│    │    └─BatchNorm2d: 3-64            --                        [128, 64, 8, 8]           [128, 64, 8, 8]           128
│    └─InvertedResidual: 2-9             --                        [128, 64, 8, 8]           [128, 64, 8, 8]           --
│    │    └─Conv1x1: 3-65                --                        [128, 64, 8, 8]           [128, 512, 8, 8]          32,768
│    │    └─BatchNorm2d: 3-66            --                        [128, 512, 8, 8]          [128, 512, 8, 8]          1,024
│    │    └─Identity: 3-67               --                        [128, 512, 8, 8]          [128, 512, 8, 8]          --
│    │    └─InfinityConv: 3-68           3                         [128, 512, 8, 8]          [128, 512, 8, 8]          4,608
│    │    └─BatchNorm2d: 3-69            --                        [128, 512, 8, 8]          [128, 512, 8, 8]          1,024
│    │    └─Identity: 3-70               --                        [128, 512, 8, 8]          [128, 512, 8, 8]          --
│    │    └─Conv1x1: 3-71                --                        [128, 512, 8, 8]          [128, 64, 8, 8]           32,768
│    │    └─BatchNorm2d: 3-72            --                        [128, 64, 8, 8]           [128, 64, 8, 8]           128
│    └─InvertedResidual: 2-10            --                        [128, 64, 8, 8]           [128, 64, 8, 8]           --
│    │    └─Conv1x1: 3-73                --                        [128, 64, 8, 8]           [128, 512, 8, 8]          32,768
│    │    └─BatchNorm2d: 3-74            --                        [128, 512, 8, 8]          [128, 512, 8, 8]          1,024
│    │    └─Identity: 3-75               --                        [128, 512, 8, 8]          [128, 512, 8, 8]          --
│    │    └─InfinityConv: 3-76           3                         [128, 512, 8, 8]          [128, 512, 8, 8]          4,608
│    │    └─BatchNorm2d: 3-77            --                        [128, 512, 8, 8]          [128, 512, 8, 8]          1,024
│    │    └─Identity: 3-78               --                        [128, 512, 8, 8]          [128, 512, 8, 8]          --
│    │    └─Conv1x1: 3-79                --                        [128, 512, 8, 8]          [128, 64, 8, 8]           32,768
│    │    └─BatchNorm2d: 3-80            --                        [128, 64, 8, 8]           [128, 64, 8, 8]           128
├─Sequential: 1-2                        --                        [128, 64]                 [128, 10]                 --
│    └─Dropout: 2-11                     --                        [128, 64]                 [128, 64]                 --
│    └─NormLinear: 2-12                  --                        [128, 64]                 [128, 10]                 650
============================================================================================================================================
Total params: 238,666
Trainable params: 238,666
Non-trainable params: 0
Total mult-adds (G): 4.60
============================================================================================================================================
Input size (MB): 1.57
Forward/backward pass size (MB): 3196.07
Params size (MB): 0.95
Estimated Total Size (MB): 3198.60
============================================================================================================================================| epoch   1 | time 24.87s | loss 1.9925 | train_acc 24.26 | val_acc 38.17 | lr 1.00e-03| best
| epoch   2 | time 24.43s | loss 1.4551 | train_acc 49.93 | val_acc 55.15 | lr 1.00e-03| best
| epoch   3 | time 24.41s | loss 1.2191 | train_acc 61.88 | val_acc 65.33 | lr 1.00e-03| best
| epoch   4 | time 24.42s | loss 1.1220 | train_acc 66.79 | val_acc 67.61 | lr 1.00e-03| best
| epoch   5 | time 24.46s | loss 1.0528 | train_acc 70.46 | val_acc 69.87 | lr 1.00e-03| best
| epoch   6 | time 24.45s | loss 1.0055 | train_acc 72.69 | val_acc 72.36 | lr 1.00e-03| best
| epoch   7 | time 24.43s | loss 0.9779 | train_acc 73.99 | val_acc 74.66 | lr 9.99e-04| best
| epoch   8 | time 24.41s | loss 0.9563 | train_acc 75.01 | val_acc 73.37 | lr 9.99e-04|
| epoch   9 | time 24.47s | loss 0.9410 | train_acc 75.48 | val_acc 75.23 | lr 9.99e-04| best
| epoch  10 | time 24.44s | loss 0.9224 | train_acc 76.44 | val_acc 74.96 | lr 9.99e-04|
| epoch  11 | time 24.44s | loss 0.9144 | train_acc 76.85 | val_acc 75.94 | lr 9.98e-04| best
| epoch  12 | time 24.48s | loss 0.9073 | train_acc 77.27 | val_acc 74.92 | lr 9.98e-04|
| epoch  13 | time 24.45s | loss 0.8931 | train_acc 77.93 | val_acc 76.78 | lr 9.98e-04| best
| epoch  14 | time 24.42s | loss 0.8859 | train_acc 78.13 | val_acc 77.13 | lr 9.97e-04| best
| epoch  15 | time 24.44s | loss 0.8761 | train_acc 78.35 | val_acc 76.39 | lr 9.97e-04|
| epoch  16 | time 24.45s | loss 0.8678 | train_acc 78.93 | val_acc 78.79 | lr 9.97e-04| best
| epoch  17 | time 24.47s | loss 0.8578 | train_acc 79.36 | val_acc 77.20 | lr 9.96e-04|
| epoch  18 | time 24.42s | loss 0.8533 | train_acc 79.67 | val_acc 77.62 | lr 9.96e-04|
| epoch  19 | time 24.45s | loss 0.8475 | train_acc 79.86 | val_acc 78.80 | lr 9.95e-04| best
| epoch  20 | time 24.47s | loss 0.8421 | train_acc 80.09 | val_acc 79.12 | lr 9.94e-04| best
| epoch  21 | time 24.45s | loss 0.8358 | train_acc 80.42 | val_acc 78.61 | lr 9.94e-04|
| epoch  22 | time 24.45s | loss 0.8318 | train_acc 80.51 | val_acc 80.83 | lr 9.93e-04| best
| epoch  23 | time 24.44s | loss 0.8291 | train_acc 80.65 | val_acc 77.63 | lr 9.93e-04|
| epoch  24 | time 24.45s | loss 0.8224 | train_acc 80.95 | val_acc 80.79 | lr 9.92e-04|
| epoch  25 | time 24.44s | loss 0.8224 | train_acc 80.93 | val_acc 80.63 | lr 9.91e-04|
| epoch  26 | time 24.45s | loss 0.8138 | train_acc 81.31 | val_acc 79.22 | lr 9.90e-04|
| epoch  27 | time 24.44s | loss 0.8109 | train_acc 81.43 | val_acc 79.49 | lr 9.90e-04|
| epoch  28 | time 24.47s | loss 0.8081 | train_acc 81.61 | val_acc 80.64 | lr 9.89e-04|
| epoch  29 | time 24.44s | loss 0.8043 | train_acc 81.77 | val_acc 79.97 | lr 9.88e-04|
| epoch  30 | time 24.46s | loss 0.7985 | train_acc 82.02 | val_acc 81.84 | lr 9.87e-04| best
| epoch  31 | time 24.43s | loss 0.7942 | train_acc 82.19 | val_acc 80.74 | lr 9.86e-04|
| epoch  32 | time 24.45s | loss 0.7935 | train_acc 82.24 | val_acc 80.33 | lr 9.85e-04|
| epoch  33 | time 24.46s | loss 0.7926 | train_acc 82.25 | val_acc 80.48 | lr 9.84e-04|
| epoch  34 | time 24.42s | loss 0.7858 | train_acc 82.50 | val_acc 81.74 | lr 9.83e-04|
| epoch  35 | time 24.40s | loss 0.7865 | train_acc 82.52 | val_acc 80.94 | lr 9.82e-04|
| epoch  36 | time 24.45s | loss 0.7805 | train_acc 82.89 | val_acc 81.99 | lr 9.81e-04| best
| epoch  37 | time 24.46s | loss 0.7816 | train_acc 82.81 | val_acc 80.83 | lr 9.80e-04|
| epoch  38 | time 24.43s | loss 0.7772 | train_acc 82.92 | val_acc 81.92 | lr 9.79e-04|
| epoch  39 | time 24.47s | loss 0.7760 | train_acc 82.92 | val_acc 81.70 | lr 9.78e-04|
| epoch  40 | time 24.40s | loss 0.7716 | train_acc 83.10 | val_acc 82.36 | lr 9.77e-04| best
| epoch  41 | time 24.42s | loss 0.7663 | train_acc 83.27 | val_acc 82.03 | lr 9.76e-04|
| epoch  42 | time 24.46s | loss 0.7654 | train_acc 83.36 | val_acc 78.86 | lr 9.74e-04|
| epoch  43 | time 24.47s | loss 0.7655 | train_acc 83.57 | val_acc 80.74 | lr 9.73e-04|
| epoch  44 | time 24.44s | loss 0.7630 | train_acc 83.54 | val_acc 82.04 | lr 9.72e-04|
| epoch  45 | time 24.46s | loss 0.7589 | train_acc 83.71 | val_acc 82.33 | lr 9.70e-04|
| epoch  46 | time 24.48s | loss 0.7614 | train_acc 83.74 | val_acc 83.11 | lr 9.69e-04| best
| epoch  47 | time 24.44s | loss 0.7535 | train_acc 83.85 | val_acc 82.17 | lr 9.68e-04|
| epoch  48 | time 24.48s | loss 0.7499 | train_acc 83.83 | val_acc 81.82 | lr 9.66e-04|
| epoch  49 | time 24.44s | loss 0.7523 | train_acc 83.75 | val_acc 81.04 | lr 9.65e-04|
| epoch  50 | time 24.43s | loss 0.7521 | train_acc 83.88 | val_acc 82.35 | lr 9.63e-04|
| epoch  51 | time 24.45s | loss 0.7518 | train_acc 83.97 | val_acc 81.74 | lr 9.62e-04|
| epoch  52 | time 24.43s | loss 0.7462 | train_acc 84.20 | val_acc 81.65 | lr 9.60e-04|
| epoch  53 | time 24.45s | loss 0.7477 | train_acc 84.12 | val_acc 82.54 | lr 9.59e-04|
| epoch  54 | time 24.47s | loss 0.7422 | train_acc 84.29 | val_acc 82.49 | lr 9.57e-04|
| epoch  55 | time 24.43s | loss 0.7402 | train_acc 84.29 | val_acc 82.65 | lr 9.56e-04|
| epoch  56 | time 24.41s | loss 0.7401 | train_acc 84.37 | val_acc 81.15 | lr 9.54e-04|
| epoch  57 | time 24.43s | loss 0.7375 | train_acc 84.82 | val_acc 83.47 | lr 9.52e-04| best
| epoch  58 | time 24.44s | loss 0.7303 | train_acc 84.96 | val_acc 83.01 | lr 9.51e-04|
| epoch  59 | time 24.46s | loss 0.7364 | train_acc 84.75 | val_acc 82.04 | lr 9.49e-04|
| epoch  60 | time 24.48s | loss 0.7319 | train_acc 84.89 | val_acc 83.13 | lr 9.47e-04|
| epoch  61 | time 24.43s | loss 0.7313 | train_acc 84.94 | val_acc 82.88 | lr 9.46e-04|
| epoch  62 | time 24.46s | loss 0.7324 | train_acc 84.78 | val_acc 83.09 | lr 9.44e-04|
| epoch  63 | time 24.42s | loss 0.7281 | train_acc 85.12 | val_acc 83.05 | lr 9.42e-04|
| epoch  64 | time 24.40s | loss 0.7256 | train_acc 85.19 | val_acc 83.48 | lr 9.40e-04| best
| epoch  65 | time 24.44s | loss 0.7301 | train_acc 84.99 | val_acc 83.57 | lr 9.38e-04| best
| epoch  66 | time 24.46s | loss 0.7253 | train_acc 85.03 | val_acc 81.19 | lr 9.36e-04|
| epoch  67 | time 24.45s | loss 0.7252 | train_acc 84.98 | val_acc 84.03 | lr 9.34e-04| best
| epoch  68 | time 24.41s | loss 0.7234 | train_acc 85.08 | val_acc 83.98 | lr 9.32e-04|
| epoch  69 | time 24.44s | loss 0.7198 | train_acc 85.34 | val_acc 84.32 | lr 9.30e-04| best
| epoch  70 | time 24.45s | loss 0.7196 | train_acc 85.27 | val_acc 81.72 | lr 9.28e-04|
| epoch  71 | time 24.44s | loss 0.7166 | train_acc 85.46 | val_acc 83.21 | lr 9.26e-04|
| epoch  72 | time 24.40s | loss 0.7170 | train_acc 85.42 | val_acc 83.44 | lr 9.24e-04|
| epoch  73 | time 24.38s | loss 0.7155 | train_acc 85.44 | val_acc 82.74 | lr 9.22e-04|
| epoch  74 | time 24.36s | loss 0.7158 | train_acc 85.45 | val_acc 84.14 | lr 9.20e-04|
| epoch  75 | time 24.37s | loss 0.7118 | train_acc 85.78 | val_acc 83.49 | lr 9.18e-04|
| epoch  76 | time 24.39s | loss 0.7124 | train_acc 85.70 | val_acc 84.05 | lr 9.16e-04|
| epoch  77 | time 24.40s | loss 0.7117 | train_acc 85.56 | val_acc 83.51 | lr 9.14e-04|
| epoch  78 | time 24.40s | loss 0.7100 | train_acc 85.62 | val_acc 84.23 | lr 9.11e-04|
| epoch  79 | time 24.38s | loss 0.7072 | train_acc 85.68 | val_acc 81.81 | lr 9.09e-04|
| epoch  80 | time 24.35s | loss 0.7059 | train_acc 85.75 | val_acc 83.54 | lr 9.07e-04|
| epoch  81 | time 24.34s | loss 0.7083 | train_acc 85.83 | val_acc 83.95 | lr 9.05e-04|
| epoch  82 | time 24.34s | loss 0.7021 | train_acc 86.17 | val_acc 83.68 | lr 9.02e-04|
| epoch  83 | time 24.37s | loss 0.6999 | train_acc 86.17 | val_acc 84.52 | lr 9.00e-04| best
| epoch  84 | time 24.35s | loss 0.6986 | train_acc 86.28 | val_acc 84.74 | lr 8.97e-04| best
| epoch  85 | time 24.38s | loss 0.7014 | train_acc 86.09 | val_acc 83.89 | lr 8.95e-04|
| epoch  86 | time 24.40s | loss 0.6984 | train_acc 86.15 | val_acc 83.90 | lr 8.93e-04|
| epoch  87 | time 24.41s | loss 0.6960 | train_acc 86.25 | val_acc 84.12 | lr 8.90e-04|
| epoch  88 | time 24.35s | loss 0.6997 | train_acc 86.03 | val_acc 84.06 | lr 8.88e-04|
| epoch  89 | time 24.34s | loss 0.6943 | train_acc 86.43 | val_acc 84.67 | lr 8.85e-04|
| epoch  90 | time 24.37s | loss 0.6958 | train_acc 86.14 | val_acc 84.30 | lr 8.83e-04|
| epoch  91 | time 24.38s | loss 0.6936 | train_acc 86.51 | val_acc 83.08 | lr 8.80e-04|
| epoch  92 | time 24.38s | loss 0.6881 | train_acc 86.72 | val_acc 84.27 | lr 8.78e-04|
| epoch  93 | time 24.35s | loss 0.6922 | train_acc 86.50 | val_acc 84.33 | lr 8.75e-04|
| epoch  94 | time 24.36s | loss 0.6922 | train_acc 86.45 | val_acc 85.39 | lr 8.72e-04| best
| epoch  95 | time 24.37s | loss 0.6880 | train_acc 86.69 | val_acc 83.35 | lr 8.70e-04|
| epoch  96 | time 24.36s | loss 0.6866 | train_acc 86.58 | val_acc 84.74 | lr 8.67e-04|
| epoch  97 | time 24.35s | loss 0.6869 | train_acc 86.68 | val_acc 84.71 | lr 8.64e-04|
| epoch  98 | time 24.38s | loss 0.6844 | train_acc 86.86 | val_acc 84.43 | lr 8.62e-04|
| epoch  99 | time 24.41s | loss 0.6852 | train_acc 86.84 | val_acc 84.88 | lr 8.59e-04|
| epoch 100 | time 24.36s | loss 0.6815 | train_acc 86.75 | val_acc 84.54 | lr 8.56e-04|
| epoch 101 | time 24.39s | loss 0.6833 | train_acc 86.80 | val_acc 84.21 | lr 8.54e-04|
| epoch 102 | time 24.38s | loss 0.6797 | train_acc 86.97 | val_acc 84.21 | lr 8.51e-04|
| epoch 103 | time 24.41s | loss 0.6826 | train_acc 86.85 | val_acc 85.51 | lr 8.48e-04| best
| epoch 104 | time 24.38s | loss 0.6805 | train_acc 87.02 | val_acc 81.89 | lr 8.45e-04|
| epoch 105 | time 24.32s | loss 0.6737 | train_acc 87.44 | val_acc 84.03 | lr 8.42e-04|
| epoch 106 | time 24.37s | loss 0.6782 | train_acc 87.00 | val_acc 85.51 | lr 8.39e-04|
| epoch 107 | time 24.38s | loss 0.6782 | train_acc 87.12 | val_acc 85.28 | lr 8.37e-04|
| epoch 108 | time 24.39s | loss 0.6725 | train_acc 87.25 | val_acc 84.70 | lr 8.34e-04|
| epoch 109 | time 24.37s | loss 0.6725 | train_acc 87.34 | val_acc 85.03 | lr 8.31e-04|
| epoch 110 | time 24.34s | loss 0.6714 | train_acc 87.38 | val_acc 84.82 | lr 8.28e-04|
| epoch 111 | time 24.39s | loss 0.6726 | train_acc 87.40 | val_acc 85.08 | lr 8.25e-04|
| epoch 112 | time 24.38s | loss 0.6696 | train_acc 87.30 | val_acc 85.39 | lr 8.22e-04|
| epoch 113 | time 24.38s | loss 0.6679 | train_acc 87.61 | val_acc 84.84 | lr 8.19e-04|
| epoch 114 | time 24.40s | loss 0.6673 | train_acc 87.47 | val_acc 85.35 | lr 8.16e-04|
| epoch 115 | time 24.33s | loss 0.6660 | train_acc 87.66 | val_acc 85.73 | lr 8.13e-04| best
| epoch 116 | time 24.38s | loss 0.6639 | train_acc 87.60 | val_acc 85.64 | lr 8.10e-04|
| epoch 117 | time 24.38s | loss 0.6642 | train_acc 87.54 | val_acc 85.83 | lr 8.06e-04| best
| epoch 118 | time 24.35s | loss 0.6634 | train_acc 87.66 | val_acc 84.98 | lr 8.03e-04|
| epoch 119 | time 24.34s | loss 0.6619 | train_acc 87.76 | val_acc 84.28 | lr 8.00e-04|
| epoch 120 | time 24.38s | loss 0.6601 | train_acc 87.92 | val_acc 85.08 | lr 7.97e-04|
| epoch 121 | time 24.37s | loss 0.6615 | train_acc 87.87 | val_acc 85.04 | lr 7.94e-04|
| epoch 122 | time 24.38s | loss 0.6549 | train_acc 87.88 | val_acc 84.79 | lr 7.91e-04|
| epoch 123 | time 24.37s | loss 0.6580 | train_acc 87.81 | val_acc 85.13 | lr 7.88e-04|
| epoch 124 | time 24.37s | loss 0.6594 | train_acc 87.71 | val_acc 85.30 | lr 7.84e-04|
| epoch 125 | time 24.34s | loss 0.6586 | train_acc 87.99 | val_acc 85.18 | lr 7.81e-04|
| epoch 126 | time 24.37s | loss 0.6551 | train_acc 88.04 | val_acc 84.40 | lr 7.78e-04|
| epoch 127 | time 24.37s | loss 0.6555 | train_acc 88.02 | val_acc 85.23 | lr 7.75e-04|
| epoch 128 | time 24.35s | loss 0.6534 | train_acc 87.95 | val_acc 86.28 | lr 7.71e-04| best
| epoch 129 | time 24.42s | loss 0.6527 | train_acc 88.01 | val_acc 85.88 | lr 7.68e-04|
| epoch 130 | time 24.41s | loss 0.6525 | train_acc 88.00 | val_acc 85.51 | lr 7.65e-04|
| epoch 131 | time 24.37s | loss 0.6515 | train_acc 88.11 | val_acc 86.14 | lr 7.61e-04|
| epoch 132 | time 24.38s | loss 0.6489 | train_acc 88.22 | val_acc 84.27 | lr 7.58e-04|
| epoch 133 | time 24.37s | loss 0.6475 | train_acc 88.43 | val_acc 85.18 | lr 7.55e-04|
| epoch 134 | time 24.38s | loss 0.6484 | train_acc 88.41 | val_acc 86.59 | lr 7.51e-04| best
| epoch 135 | time 24.39s | loss 0.6480 | train_acc 88.27 | val_acc 85.68 | lr 7.48e-04|
| epoch 136 | time 24.38s | loss 0.6442 | train_acc 88.42 | val_acc 86.30 | lr 7.44e-04|
| epoch 137 | time 24.38s | loss 0.6471 | train_acc 88.23 | val_acc 86.25 | lr 7.41e-04|
| epoch 138 | time 24.37s | loss 0.6412 | train_acc 88.57 | val_acc 85.09 | lr 7.37e-04|
| epoch 139 | time 24.35s | loss 0.6422 | train_acc 88.58 | val_acc 86.48 | lr 7.34e-04|
| epoch 140 | time 24.35s | loss 0.6417 | train_acc 88.38 | val_acc 85.95 | lr 7.30e-04|
| epoch 141 | time 24.34s | loss 0.6363 | train_acc 88.79 | val_acc 86.09 | lr 7.27e-04|
| epoch 142 | time 24.36s | loss 0.6376 | train_acc 88.53 | val_acc 86.54 | lr 7.23e-04|
| epoch 143 | time 24.37s | loss 0.6393 | train_acc 88.68 | val_acc 85.24 | lr 7.20e-04|
| epoch 144 | time 24.37s | loss 0.6368 | train_acc 88.71 | val_acc 85.60 | lr 7.16e-04|
| epoch 145 | time 24.37s | loss 0.6347 | train_acc 88.87 | val_acc 86.54 | lr 7.13e-04|
| epoch 146 | time 24.36s | loss 0.6341 | train_acc 88.77 | val_acc 86.55 | lr 7.09e-04|
| epoch 147 | time 24.39s | loss 0.6333 | train_acc 88.77 | val_acc 87.04 | lr 7.06e-04| best
| epoch 148 | time 24.36s | loss 0.6365 | train_acc 88.76 | val_acc 85.89 | lr 7.02e-04|
| epoch 149 | time 24.37s | loss 0.6288 | train_acc 89.17 | val_acc 86.47 | lr 6.99e-04|
| epoch 150 | time 24.37s | loss 0.6310 | train_acc 89.06 | val_acc 86.18 | lr 6.95e-04|
| epoch 151 | time 24.38s | loss 0.6281 | train_acc 89.17 | val_acc 86.41 | lr 6.91e-04|
| epoch 152 | time 24.43s | loss 0.6265 | train_acc 89.31 | val_acc 87.00 | lr 6.88e-04|
| epoch 153 | time 24.37s | loss 0.6272 | train_acc 89.31 | val_acc 86.48 | lr 6.84e-04|
| epoch 154 | time 24.38s | loss 0.6253 | train_acc 89.02 | val_acc 86.92 | lr 6.80e-04|
| epoch 155 | time 24.38s | loss 0.6267 | train_acc 89.10 | val_acc 85.88 | lr 6.77e-04|
| epoch 156 | time 24.34s | loss 0.6238 | train_acc 89.34 | val_acc 86.33 | lr 6.73e-04|
| epoch 157 | time 24.36s | loss 0.6222 | train_acc 89.38 | val_acc 86.31 | lr 6.69e-04|
| epoch 158 | time 24.34s | loss 0.6194 | train_acc 89.43 | val_acc 86.52 | lr 6.66e-04|
| epoch 159 | time 24.39s | loss 0.6213 | train_acc 89.27 | val_acc 86.48 | lr 6.62e-04|
| epoch 160 | time 24.36s | loss 0.6171 | train_acc 89.48 | val_acc 86.58 | lr 6.58e-04|
| epoch 161 | time 24.37s | loss 0.6193 | train_acc 89.32 | val_acc 86.53 | lr 6.55e-04|
| epoch 162 | time 24.36s | loss 0.6185 | train_acc 89.38 | val_acc 86.64 | lr 6.51e-04|
| epoch 163 | time 24.37s | loss 0.6147 | train_acc 89.80 | val_acc 86.41 | lr 6.47e-04|
| epoch 164 | time 24.37s | loss 0.6141 | train_acc 89.77 | val_acc 86.70 | lr 6.43e-04|
| epoch 165 | time 24.32s | loss 0.6122 | train_acc 89.87 | val_acc 87.26 | lr 6.39e-04| best
| epoch 166 | time 24.34s | loss 0.6159 | train_acc 89.59 | val_acc 87.23 | lr 6.36e-04|
| epoch 167 | time 24.36s | loss 0.6092 | train_acc 89.84 | val_acc 86.09 | lr 6.32e-04|
| epoch 168 | time 24.36s | loss 0.6138 | train_acc 89.50 | val_acc 85.22 | lr 6.28e-04|
| epoch 169 | time 24.33s | loss 0.6078 | train_acc 90.08 | val_acc 86.57 | lr 6.24e-04|
| epoch 170 | time 24.38s | loss 0.6100 | train_acc 89.76 | val_acc 86.99 | lr 6.21e-04|
| epoch 171 | time 24.37s | loss 0.6084 | train_acc 89.81 | val_acc 86.25 | lr 6.17e-04|
| epoch 172 | time 24.34s | loss 0.6037 | train_acc 90.19 | val_acc 87.26 | lr 6.13e-04|
| epoch 173 | time 24.36s | loss 0.6058 | train_acc 89.92 | val_acc 86.67 | lr 6.09e-04|
| epoch 174 | time 24.32s | loss 0.6029 | train_acc 90.24 | val_acc 87.31 | lr 6.05e-04| best
| epoch 175 | time 24.35s | loss 0.6026 | train_acc 90.08 | val_acc 86.67 | lr 6.01e-04|
| epoch 176 | time 24.35s | loss 0.6008 | train_acc 90.23 | val_acc 86.88 | lr 5.98e-04|
| epoch 177 | time 24.37s | loss 0.5991 | train_acc 90.47 | val_acc 87.33 | lr 5.94e-04| best
| epoch 178 | time 24.34s | loss 0.5985 | train_acc 90.29 | val_acc 87.81 | lr 5.90e-04| best
| epoch 179 | time 24.35s | loss 0.6020 | train_acc 90.18 | val_acc 87.25 | lr 5.86e-04|
| epoch 180 | time 24.35s | loss 0.5968 | train_acc 90.48 | val_acc 87.31 | lr 5.82e-04|
| epoch 181 | time 24.35s | loss 0.5957 | train_acc 90.42 | val_acc 87.17 | lr 5.78e-04|
| epoch 182 | time 24.35s | loss 0.5937 | train_acc 90.42 | val_acc 87.33 | lr 5.74e-04|
| epoch 183 | time 24.37s | loss 0.5935 | train_acc 90.58 | val_acc 87.75 | lr 5.70e-04|
| epoch 184 | time 24.34s | loss 0.5937 | train_acc 90.45 | val_acc 87.40 | lr 5.67e-04|
| epoch 185 | time 24.36s | loss 0.5927 | train_acc 90.48 | val_acc 86.87 | lr 5.63e-04|
| epoch 186 | time 24.36s | loss 0.5908 | train_acc 90.64 | val_acc 87.03 | lr 5.59e-04|
| epoch 187 | time 24.37s | loss 0.5884 | train_acc 90.72 | val_acc 86.63 | lr 5.55e-04|
| epoch 188 | time 24.37s | loss 0.5889 | train_acc 90.74 | val_acc 87.77 | lr 5.51e-04|
| epoch 189 | time 24.37s | loss 0.5898 | train_acc 90.53 | val_acc 86.91 | lr 5.47e-04|
| epoch 190 | time 24.34s | loss 0.5863 | train_acc 90.63 | val_acc 87.76 | lr 5.43e-04|
| epoch 191 | time 24.38s | loss 0.5846 | train_acc 90.93 | val_acc 87.95 | lr 5.39e-04| best
| epoch 192 | time 24.37s | loss 0.5832 | train_acc 90.99 | val_acc 87.83 | lr 5.35e-04|
| epoch 193 | time 24.35s | loss 0.5841 | train_acc 90.81 | val_acc 87.73 | lr 5.31e-04|
| epoch 194 | time 24.35s | loss 0.5814 | train_acc 91.05 | val_acc 88.33 | lr 5.27e-04| best
| epoch 195 | time 24.36s | loss 0.5789 | train_acc 91.01 | val_acc 88.60 | lr 5.24e-04| best
| epoch 196 | time 24.35s | loss 0.5806 | train_acc 91.02 | val_acc 87.70 | lr 5.20e-04|
| epoch 197 | time 24.35s | loss 0.5781 | train_acc 91.23 | val_acc 88.25 | lr 5.16e-04|
| epoch 198 | time 24.36s | loss 0.5750 | train_acc 91.44 | val_acc 87.46 | lr 5.12e-04|
| epoch 199 | time 24.38s | loss 0.5771 | train_acc 91.19 | val_acc 87.67 | lr 5.08e-04|
| epoch 200 | time 24.33s | loss 0.5763 | train_acc 91.25 | val_acc 88.13 | lr 5.04e-04|
| epoch 201 | time 24.34s | loss 0.5772 | train_acc 91.22 | val_acc 87.72 | lr 5.00e-04|
| epoch 202 | time 24.34s | loss 0.5741 | train_acc 91.24 | val_acc 87.66 | lr 4.96e-04|
| epoch 203 | time 24.36s | loss 0.5731 | train_acc 91.39 | val_acc 88.68 | lr 4.92e-04| best
| epoch 204 | time 24.33s | loss 0.5717 | train_acc 91.52 | val_acc 88.12 | lr 4.88e-04|
| epoch 205 | time 24.37s | loss 0.5678 | train_acc 91.60 | val_acc 87.48 | lr 4.84e-04|
| epoch 206 | time 24.34s | loss 0.5688 | train_acc 91.55 | val_acc 87.67 | lr 4.80e-04|
| epoch 207 | time 24.38s | loss 0.5667 | train_acc 91.63 | val_acc 88.25 | lr 4.76e-04|
| epoch 208 | time 24.36s | loss 0.5659 | train_acc 91.60 | val_acc 88.41 | lr 4.73e-04|
| epoch 209 | time 24.37s | loss 0.5648 | train_acc 91.73 | val_acc 87.81 | lr 4.69e-04|
| epoch 210 | time 24.38s | loss 0.5636 | train_acc 91.80 | val_acc 88.65 | lr 4.65e-04|
| epoch 211 | time 24.38s | loss 0.5623 | train_acc 91.79 | val_acc 88.12 | lr 4.61e-04|
| epoch 212 | time 24.35s | loss 0.5623 | train_acc 91.72 | val_acc 87.45 | lr 4.57e-04|
| epoch 213 | time 24.37s | loss 0.5590 | train_acc 92.00 | val_acc 87.94 | lr 4.53e-04|
| epoch 214 | time 24.38s | loss 0.5595 | train_acc 91.91 | val_acc 88.00 | lr 4.49e-04|
| epoch 215 | time 24.35s | loss 0.5594 | train_acc 91.99 | val_acc 88.36 | lr 4.45e-04|
| epoch 216 | time 24.36s | loss 0.5560 | train_acc 92.11 | val_acc 88.04 | lr 4.41e-04|
| epoch 217 | time 24.38s | loss 0.5552 | train_acc 92.10 | val_acc 88.79 | lr 4.37e-04| best
| epoch 218 | time 24.37s | loss 0.5545 | train_acc 92.13 | val_acc 88.11 | lr 4.33e-04|
| epoch 219 | time 24.34s | loss 0.5542 | train_acc 92.15 | val_acc 88.50 | lr 4.30e-04|
| epoch 220 | time 24.34s | loss 0.5493 | train_acc 92.40 | val_acc 88.07 | lr 4.26e-04|
| epoch 221 | time 24.35s | loss 0.5494 | train_acc 92.34 | val_acc 88.20 | lr 4.22e-04|
| epoch 222 | time 24.36s | loss 0.5500 | train_acc 92.44 | val_acc 88.15 | lr 4.18e-04|
| epoch 223 | time 24.36s | loss 0.5485 | train_acc 92.45 | val_acc 88.43 | lr 4.14e-04|
| epoch 224 | time 24.36s | loss 0.5465 | train_acc 92.41 | val_acc 88.07 | lr 4.10e-04|
| epoch 225 | time 24.37s | loss 0.5465 | train_acc 92.49 | val_acc 88.68 | lr 4.06e-04|
| epoch 226 | time 24.38s | loss 0.5433 | train_acc 92.55 | val_acc 88.77 | lr 4.02e-04|
| epoch 227 | time 24.36s | loss 0.5440 | train_acc 92.54 | val_acc 87.95 | lr 3.99e-04|
| epoch 228 | time 24.36s | loss 0.5439 | train_acc 92.53 | val_acc 89.26 | lr 3.95e-04| best
| epoch 229 | time 24.36s | loss 0.5406 | train_acc 92.68 | val_acc 88.62 | lr 3.91e-04|
| epoch 230 | time 24.35s | loss 0.5403 | train_acc 92.76 | val_acc 89.03 | lr 3.87e-04|
| epoch 231 | time 24.36s | loss 0.5376 | train_acc 92.90 | val_acc 88.88 | lr 3.83e-04|
| epoch 232 | time 24.36s | loss 0.5375 | train_acc 92.89 | val_acc 88.83 | lr 3.79e-04|
| epoch 233 | time 24.35s | loss 0.5341 | train_acc 93.10 | val_acc 89.11 | lr 3.76e-04|
| epoch 234 | time 24.34s | loss 0.5344 | train_acc 92.94 | val_acc 88.33 | lr 3.72e-04|
| epoch 235 | time 24.34s | loss 0.5340 | train_acc 93.02 | val_acc 89.12 | lr 3.68e-04|
| epoch 236 | time 24.33s | loss 0.5315 | train_acc 93.12 | val_acc 88.73 | lr 3.64e-04|
| epoch 237 | time 24.34s | loss 0.5304 | train_acc 93.13 | val_acc 88.69 | lr 3.61e-04|
| epoch 238 | time 24.37s | loss 0.5314 | train_acc 93.11 | val_acc 88.85 | lr 3.57e-04|
| epoch 239 | time 24.33s | loss 0.5291 | train_acc 93.22 | val_acc 88.89 | lr 3.53e-04|
| epoch 240 | time 24.33s | loss 0.5248 | train_acc 93.29 | val_acc 88.86 | lr 3.49e-04|
| epoch 241 | time 24.34s | loss 0.5265 | train_acc 93.31 | val_acc 89.10 | lr 3.45e-04|
| epoch 242 | time 24.36s | loss 0.5236 | train_acc 93.54 | val_acc 88.94 | lr 3.42e-04|
| epoch 243 | time 24.32s | loss 0.5234 | train_acc 93.48 | val_acc 89.21 | lr 3.38e-04|
| epoch 244 | time 24.36s | loss 0.5206 | train_acc 93.50 | val_acc 88.53 | lr 3.34e-04|
| epoch 245 | time 24.34s | loss 0.5207 | train_acc 93.64 | val_acc 89.20 | lr 3.31e-04|
| epoch 246 | time 24.34s | loss 0.5204 | train_acc 93.57 | val_acc 88.80 | lr 3.27e-04|
| epoch 247 | time 24.31s | loss 0.5176 | train_acc 93.72 | val_acc 88.53 | lr 3.23e-04|
| epoch 248 | time 24.36s | loss 0.5181 | train_acc 93.74 | val_acc 89.17 | lr 3.20e-04|
| epoch 249 | time 24.38s | loss 0.5164 | train_acc 93.67 | val_acc 89.06 | lr 3.16e-04|
| epoch 250 | time 24.32s | loss 0.5157 | train_acc 93.75 | val_acc 88.99 | lr 3.12e-04|
| epoch 251 | time 24.37s | loss 0.5148 | train_acc 93.85 | val_acc 88.69 | lr 3.09e-04|
| epoch 252 | time 24.32s | loss 0.5157 | train_acc 93.84 | val_acc 88.84 | lr 3.05e-04|
| epoch 253 | time 24.38s | loss 0.5092 | train_acc 94.15 | val_acc 89.12 | lr 3.01e-04|
| epoch 254 | time 24.38s | loss 0.5111 | train_acc 93.93 | val_acc 88.95 | lr 2.98e-04|
| epoch 255 | time 24.37s | loss 0.5079 | train_acc 94.26 | val_acc 89.09 | lr 2.94e-04|
| epoch 256 | time 24.34s | loss 0.5090 | train_acc 94.01 | val_acc 89.72 | lr 2.91e-04| best
| epoch 257 | time 24.37s | loss 0.5057 | train_acc 94.19 | val_acc 89.02 | lr 2.87e-04|
| epoch 258 | time 24.36s | loss 0.5055 | train_acc 94.21 | val_acc 89.03 | lr 2.84e-04|
| epoch 259 | time 24.33s | loss 0.5019 | train_acc 94.47 | val_acc 89.44 | lr 2.80e-04|
| epoch 260 | time 24.33s | loss 0.5020 | train_acc 94.34 | val_acc 89.13 | lr 2.77e-04|
| epoch 261 | time 24.34s | loss 0.5021 | train_acc 94.31 | val_acc 89.18 | lr 2.73e-04|
| epoch 262 | time 24.33s | loss 0.5000 | train_acc 94.44 | val_acc 89.26 | lr 2.70e-04|
| epoch 263 | time 24.37s | loss 0.4998 | train_acc 94.48 | val_acc 88.34 | lr 2.66e-04|
| epoch 264 | time 24.35s | loss 0.4975 | train_acc 94.63 | val_acc 89.08 | lr 2.63e-04|
| epoch 265 | time 24.35s | loss 0.4976 | train_acc 94.50 | val_acc 89.45 | lr 2.59e-04|
| epoch 266 | time 24.36s | loss 0.4948 | train_acc 94.64 | val_acc 89.71 | lr 2.56e-04|
| epoch 267 | time 24.37s | loss 0.4936 | train_acc 94.73 | val_acc 89.15 | lr 2.52e-04|
| epoch 268 | time 24.36s | loss 0.4936 | train_acc 94.77 | val_acc 89.58 | lr 2.49e-04|
| epoch 269 | time 24.36s | loss 0.4916 | train_acc 94.76 | val_acc 89.35 | lr 2.45e-04|
| epoch 270 | time 24.38s | loss 0.4884 | train_acc 94.94 | val_acc 89.23 | lr 2.42e-04|
| epoch 271 | time 24.36s | loss 0.4888 | train_acc 94.98 | val_acc 89.69 | lr 2.39e-04|
| epoch 272 | time 24.34s | loss 0.4905 | train_acc 94.86 | val_acc 89.21 | lr 2.35e-04|
| epoch 273 | time 24.37s | loss 0.4878 | train_acc 95.05 | val_acc 89.24 | lr 2.32e-04|
| epoch 274 | time 24.38s | loss 0.4852 | train_acc 95.12 | val_acc 89.41 | lr 2.29e-04|
| epoch 275 | time 24.35s | loss 0.4832 | train_acc 95.17 | val_acc 89.76 | lr 2.25e-04| best
| epoch 276 | time 24.36s | loss 0.4832 | train_acc 95.26 | val_acc 89.75 | lr 2.22e-04|
| epoch 277 | time 24.37s | loss 0.4815 | train_acc 95.26 | val_acc 89.37 | lr 2.19e-04|
| epoch 278 | time 24.35s | loss 0.4784 | train_acc 95.35 | val_acc 89.49 | lr 2.16e-04|
| epoch 279 | time 24.36s | loss 0.4789 | train_acc 95.32 | val_acc 89.26 | lr 2.12e-04|
| epoch 280 | time 24.40s | loss 0.4784 | train_acc 95.38 | val_acc 89.68 | lr 2.09e-04|
| epoch 281 | time 24.35s | loss 0.4756 | train_acc 95.55 | val_acc 89.99 | lr 2.06e-04| best
| epoch 282 | time 24.38s | loss 0.4745 | train_acc 95.64 | val_acc 89.52 | lr 2.03e-04|
| epoch 283 | time 24.35s | loss 0.4750 | train_acc 95.50 | val_acc 89.93 | lr 2.00e-04|
| epoch 284 | time 24.36s | loss 0.4731 | train_acc 95.53 | val_acc 89.91 | lr 1.97e-04|
| epoch 285 | time 24.33s | loss 0.4720 | train_acc 95.60 | val_acc 89.95 | lr 1.94e-04|
| epoch 286 | time 24.35s | loss 0.4693 | train_acc 95.77 | val_acc 89.53 | lr 1.90e-04|
| epoch 287 | time 24.35s | loss 0.4713 | train_acc 95.66 | val_acc 89.70 | lr 1.87e-04|
| epoch 288 | time 24.37s | loss 0.4657 | train_acc 95.98 | val_acc 89.69 | lr 1.84e-04|
| epoch 289 | time 24.37s | loss 0.4671 | train_acc 95.79 | val_acc 89.75 | lr 1.81e-04|
| epoch 290 | time 24.38s | loss 0.4649 | train_acc 96.08 | val_acc 90.02 | lr 1.78e-04| best
| epoch 291 | time 24.35s | loss 0.4643 | train_acc 95.99 | val_acc 89.73 | lr 1.75e-04|
| epoch 292 | time 24.34s | loss 0.4637 | train_acc 96.13 | val_acc 90.08 | lr 1.72e-04| best
| epoch 293 | time 24.37s | loss 0.4607 | train_acc 96.11 | val_acc 89.92 | lr 1.69e-04|
| epoch 294 | time 24.33s | loss 0.4613 | train_acc 96.18 | val_acc 90.23 | lr 1.66e-04| best
| epoch 295 | time 24.37s | loss 0.4597 | train_acc 96.09 | val_acc 89.97 | lr 1.63e-04|
| epoch 296 | time 24.37s | loss 0.4579 | train_acc 96.18 | val_acc 89.74 | lr 1.61e-04|
| epoch 297 | time 24.35s | loss 0.4580 | train_acc 96.25 | val_acc 89.82 | lr 1.58e-04|
| epoch 298 | time 24.38s | loss 0.4571 | train_acc 96.21 | val_acc 89.94 | lr 1.55e-04|
| epoch 299 | time 24.39s | loss 0.4570 | train_acc 96.30 | val_acc 89.88 | lr 1.52e-04|
| epoch 300 | time 24.38s | loss 0.4527 | train_acc 96.50 | val_acc 90.18 | lr 1.49e-04|
| epoch 301 | time 24.37s | loss 0.4528 | train_acc 96.45 | val_acc 89.99 | lr 1.46e-04|
| epoch 302 | time 24.36s | loss 0.4521 | train_acc 96.42 | val_acc 89.86 | lr 1.44e-04|
| epoch 303 | time 24.38s | loss 0.4519 | train_acc 96.51 | val_acc 90.03 | lr 1.41e-04|
| epoch 304 | time 24.37s | loss 0.4502 | train_acc 96.63 | val_acc 89.86 | lr 1.38e-04|
| epoch 305 | time 24.34s | loss 0.4488 | train_acc 96.67 | val_acc 90.16 | lr 1.36e-04|
| epoch 306 | time 24.38s | loss 0.4479 | train_acc 96.72 | val_acc 90.00 | lr 1.33e-04|
| epoch 307 | time 24.37s | loss 0.4474 | train_acc 96.68 | val_acc 90.10 | lr 1.30e-04|
| epoch 308 | time 24.38s | loss 0.4472 | train_acc 96.73 | val_acc 90.21 | lr 1.28e-04|
| epoch 309 | time 24.36s | loss 0.4461 | train_acc 96.77 | val_acc 90.11 | lr 1.25e-04|
| epoch 310 | time 24.36s | loss 0.4451 | train_acc 96.77 | val_acc 90.35 | lr 1.22e-04| best
| epoch 311 | time 24.36s | loss 0.4407 | train_acc 97.01 | val_acc 90.34 | lr 1.20e-04|
| epoch 312 | time 24.36s | loss 0.4415 | train_acc 97.00 | val_acc 90.24 | lr 1.17e-04|
| epoch 313 | time 24.34s | loss 0.4406 | train_acc 97.01 | val_acc 90.14 | lr 1.15e-04|
| epoch 314 | time 24.40s | loss 0.4400 | train_acc 97.11 | val_acc 90.15 | lr 1.12e-04|
| epoch 315 | time 24.36s | loss 0.4377 | train_acc 97.12 | val_acc 90.27 | lr 1.10e-04|
| epoch 316 | time 24.33s | loss 0.4377 | train_acc 97.12 | val_acc 89.91 | lr 1.07e-04|
| epoch 317 | time 24.35s | loss 0.4366 | train_acc 97.16 | val_acc 90.52 | lr 1.05e-04| best
| epoch 318 | time 24.36s | loss 0.4371 | train_acc 97.13 | val_acc 90.24 | lr 1.03e-04|
| epoch 319 | time 24.39s | loss 0.4349 | train_acc 97.13 | val_acc 90.21 | lr 1.00e-04|
| epoch 320 | time 24.35s | loss 0.4335 | train_acc 97.30 | val_acc 90.33 | lr 9.78e-05|
| epoch 321 | time 24.37s | loss 0.4335 | train_acc 97.35 | val_acc 90.40 | lr 9.55e-05|
| epoch 322 | time 24.40s | loss 0.4326 | train_acc 97.28 | val_acc 90.63 | lr 9.32e-05| best
| epoch 323 | time 24.35s | loss 0.4305 | train_acc 97.37 | val_acc 90.45 | lr 9.09e-05|
| epoch 324 | time 24.35s | loss 0.4299 | train_acc 97.50 | val_acc 90.67 | lr 8.87e-05| best
| epoch 325 | time 24.38s | loss 0.4278 | train_acc 97.50 | val_acc 90.41 | lr 8.65e-05|
| epoch 326 | time 24.34s | loss 0.4279 | train_acc 97.49 | val_acc 90.70 | lr 8.43e-05| best
| epoch 327 | time 24.35s | loss 0.4273 | train_acc 97.45 | val_acc 90.40 | lr 8.21e-05|
| epoch 328 | time 24.38s | loss 0.4270 | train_acc 97.56 | val_acc 90.44 | lr 8.00e-05|
| epoch 329 | time 24.38s | loss 0.4271 | train_acc 97.61 | val_acc 89.93 | lr 7.78e-05|
| epoch 330 | time 24.36s | loss 0.4252 | train_acc 97.61 | val_acc 90.58 | lr 7.57e-05|
| epoch 331 | time 24.37s | loss 0.4243 | train_acc 97.74 | val_acc 90.40 | lr 7.37e-05|
| epoch 332 | time 24.36s | loss 0.4256 | train_acc 97.63 | val_acc 90.41 | lr 7.16e-05|
| epoch 333 | time 24.39s | loss 0.4243 | train_acc 97.64 | val_acc 90.45 | lr 6.96e-05|
| epoch 334 | time 24.34s | loss 0.4221 | train_acc 97.70 | val_acc 90.16 | lr 6.76e-05|
| epoch 335 | time 24.35s | loss 0.4197 | train_acc 97.90 | val_acc 90.60 | lr 6.57e-05|
| epoch 336 | time 24.41s | loss 0.4209 | train_acc 97.73 | val_acc 90.51 | lr 6.38e-05|
| epoch 337 | time 24.37s | loss 0.4194 | train_acc 97.87 | val_acc 90.45 | lr 6.18e-05|
| epoch 338 | time 24.36s | loss 0.4184 | train_acc 97.95 | val_acc 90.61 | lr 6.00e-05|
| epoch 339 | time 24.35s | loss 0.4175 | train_acc 97.89 | val_acc 90.60 | lr 5.81e-05|
| epoch 340 | time 24.36s | loss 0.4173 | train_acc 97.95 | val_acc 90.62 | lr 5.63e-05|
| epoch 341 | time 24.35s | loss 0.4161 | train_acc 98.03 | val_acc 90.60 | lr 5.45e-05|
| epoch 342 | time 24.37s | loss 0.4147 | train_acc 98.07 | val_acc 90.49 | lr 5.27e-05|
| epoch 343 | time 24.35s | loss 0.4157 | train_acc 98.02 | val_acc 90.31 | lr 5.10e-05|
| epoch 344 | time 24.38s | loss 0.4144 | train_acc 98.07 | val_acc 90.38 | lr 4.93e-05|
| epoch 345 | time 24.39s | loss 0.4141 | train_acc 98.04 | val_acc 90.29 | lr 4.76e-05|
| epoch 346 | time 24.37s | loss 0.4115 | train_acc 98.20 | val_acc 90.26 | lr 4.59e-05|
| epoch 347 | time 24.37s | loss 0.4126 | train_acc 98.09 | val_acc 90.54 | lr 4.43e-05|
| epoch 348 | time 24.37s | loss 0.4121 | train_acc 98.13 | val_acc 90.65 | lr 4.27e-05|
| epoch 349 | time 24.36s | loss 0.4123 | train_acc 98.15 | val_acc 90.60 | lr 4.11e-05|
| epoch 350 | time 24.38s | loss 0.4112 | train_acc 98.16 | val_acc 90.64 | lr 3.96e-05|
| epoch 351 | time 24.36s | loss 0.4101 | train_acc 98.18 | val_acc 90.46 | lr 3.81e-05|
| epoch 352 | time 24.36s | loss 0.4097 | train_acc 98.25 | val_acc 90.49 | lr 3.66e-05|
| epoch 353 | time 24.36s | loss 0.4082 | train_acc 98.37 | val_acc 90.56 | lr 3.51e-05|
| epoch 354 | time 24.35s | loss 0.4093 | train_acc 98.22 | val_acc 90.55 | lr 3.37e-05|
| epoch 355 | time 24.34s | loss 0.4088 | train_acc 98.29 | val_acc 90.59 | lr 3.23e-05|
| epoch 356 | time 24.33s | loss 0.4076 | train_acc 98.35 | val_acc 90.62 | lr 3.09e-05|
| epoch 357 | time 24.37s | loss 0.4086 | train_acc 98.30 | val_acc 90.67 | lr 2.96e-05|
| epoch 358 | time 24.39s | loss 0.4058 | train_acc 98.39 | val_acc 90.64 | lr 2.82e-05|
| epoch 359 | time 24.38s | loss 0.4054 | train_acc 98.42 | val_acc 90.62 | lr 2.70e-05|
| epoch 360 | time 24.33s | loss 0.4062 | train_acc 98.38 | val_acc 90.68 | lr 2.57e-05|
| epoch 361 | time 24.37s | loss 0.4061 | train_acc 98.35 | val_acc 90.80 | lr 2.45e-05| best
| epoch 362 | time 24.33s | loss 0.4055 | train_acc 98.41 | val_acc 90.56 | lr 2.33e-05|
| epoch 363 | time 24.37s | loss 0.4047 | train_acc 98.41 | val_acc 90.48 | lr 2.21e-05|
| epoch 364 | time 24.33s | loss 0.4050 | train_acc 98.40 | val_acc 90.60 | lr 2.10e-05|
| epoch 365 | time 24.36s | loss 0.4046 | train_acc 98.42 | val_acc 90.54 | lr 1.99e-05|
| epoch 366 | time 24.38s | loss 0.4030 | train_acc 98.42 | val_acc 90.50 | lr 1.88e-05|
| epoch 367 | time 24.37s | loss 0.4038 | train_acc 98.49 | val_acc 90.60 | lr 1.77e-05|
| epoch 368 | time 24.35s | loss 0.4028 | train_acc 98.55 | val_acc 90.72 | lr 1.67e-05|
| epoch 369 | time 24.39s | loss 0.4022 | train_acc 98.53 | val_acc 90.70 | lr 1.57e-05|
| epoch 370 | time 24.37s | loss 0.4032 | train_acc 98.53 | val_acc 90.63 | lr 1.47e-05|
| epoch 371 | time 24.40s | loss 0.4014 | train_acc 98.61 | val_acc 90.63 | lr 1.38e-05|
| epoch 372 | time 24.37s | loss 0.4022 | train_acc 98.54 | val_acc 90.70 | lr 1.29e-05|
| epoch 373 | time 24.38s | loss 0.4009 | train_acc 98.59 | val_acc 90.80 | lr 1.20e-05|
| epoch 374 | time 24.36s | loss 0.4017 | train_acc 98.58 | val_acc 90.78 | lr 1.12e-05|
| epoch 375 | time 24.34s | loss 0.4004 | train_acc 98.62 | val_acc 90.72 | lr 1.04e-05|
| epoch 376 | time 24.35s | loss 0.3995 | train_acc 98.58 | val_acc 90.77 | lr 9.61e-06|
| epoch 377 | time 24.37s | loss 0.4001 | train_acc 98.62 | val_acc 90.62 | lr 8.86e-06|
| epoch 378 | time 24.39s | loss 0.4004 | train_acc 98.62 | val_acc 90.69 | lr 8.14e-06|
| epoch 379 | time 24.36s | loss 0.4009 | train_acc 98.56 | val_acc 90.62 | lr 7.45e-06|
| epoch 380 | time 24.34s | loss 0.4002 | train_acc 98.66 | val_acc 90.78 | lr 6.79e-06|
| epoch 381 | time 24.36s | loss 0.3996 | train_acc 98.62 | val_acc 90.78 | lr 6.16e-06|
| epoch 382 | time 24.38s | loss 0.3989 | train_acc 98.65 | val_acc 90.66 | lr 5.56e-06|
| epoch 383 | time 24.35s | loss 0.3989 | train_acc 98.72 | val_acc 90.75 | lr 4.99e-06|
| epoch 384 | time 24.33s | loss 0.3990 | train_acc 98.59 | val_acc 90.65 | lr 4.45e-06|
| epoch 385 | time 24.35s | loss 0.3989 | train_acc 98.69 | val_acc 90.63 | lr 3.94e-06|
| epoch 386 | time 24.34s | loss 0.3984 | train_acc 98.64 | val_acc 90.62 | lr 3.47e-06|
| epoch 387 | time 24.39s | loss 0.3983 | train_acc 98.65 | val_acc 90.70 | lr 3.02e-06|
| epoch 388 | time 24.34s | loss 0.3986 | train_acc 98.67 | val_acc 90.66 | lr 2.60e-06|
| epoch 389 | time 24.35s | loss 0.3989 | train_acc 98.68 | val_acc 90.78 | lr 2.22e-06|
| epoch 390 | time 24.35s | loss 0.3990 | train_acc 98.65 | val_acc 90.72 | lr 1.86e-06|
| epoch 391 | time 24.32s | loss 0.3976 | train_acc 98.71 | val_acc 90.57 | lr 1.54e-06|
| epoch 392 | time 24.35s | loss 0.3986 | train_acc 98.64 | val_acc 90.64 | lr 1.25e-06|
| epoch 393 | time 24.37s | loss 0.3971 | train_acc 98.72 | val_acc 90.68 | lr 9.87e-07|
| epoch 394 | time 24.33s | loss 0.3976 | train_acc 98.67 | val_acc 90.69 | lr 7.55e-07|
| epoch 395 | time 24.37s | loss 0.3973 | train_acc 98.66 | val_acc 90.72 | lr 5.55e-07|
| epoch 396 | time 24.34s | loss 0.3978 | train_acc 98.69 | val_acc 90.78 | lr 3.85e-07|
| epoch 397 | time 24.35s | loss 0.3985 | train_acc 98.60 | val_acc 90.74 | lr 2.47e-07|
| epoch 398 | time 24.36s | loss 0.3976 | train_acc 98.69 | val_acc 90.70 | lr 1.39e-07|
| epoch 399 | time 24.33s | loss 0.3993 | train_acc 98.64 | val_acc 90.87 | lr 6.17e-08| best
| epoch 400 | time 24.31s | loss 0.3981 | train_acc 98.72 | val_acc 90.81 | lr 1.54e-08|
